The results may be grouped in three groups. The models that used LSTM without a convolutional layer performed poorly. The models with convolutional layers that used LSTM as the final layer had intermediary results. The remaining models were the single layer perceptron (single fully-connected layer model, identified as ``D'') and the models that used convolutional models and used a fully-connected layer or maxpooling as the last layer.

The best accuracy results were from the three models identified as ``CCM'', ``CM'', and ``CLD'', but the latter had faster training time and also showed good resilience to changes: during preliminary tests and later when test conditions where altered to try to improve results, this model and its variations always were among the best models while the others were not.

This study evaluated some alternative models in the file fragment classification task. The expectation was to identify the most promising models for improvement. But an apparent limit was found on how far these models could be improved. 

The anticipation that LSTM models would be a good fit to file fragment classification was contradicted: the results of networks using only LSTM layers without convolutional ones were poor in accuracy and training speed. However, good results were obtained using LSTM as an auxiliary layer to a mainly convolutional neural network (CNN). 

% Using different combinations of layers and parameters, the expectation was that some models would be discarded and some would be selected as promising alternatives. That could be used to guide future researches and also serve as a reference for comparison between studies. 
The Govdocs1 dataset brought an important basis for comparison to be used between carving solutions. But to achieve easily reproducible results, the models must also be publicly available, a condition not all revised studies fulfill. Being available as Jupyter notebooks at https://github.com/atilaromero/carving-experiments, the results described here should require little effort to be reproduced. Thus the source code to generate models with same architecture of those presented here can be used as a basis of comparison in future researches.

\section{Limitations and threats to validity}
In this research, the models were not trained until exhaustion. This was initially done to identify which models would be most promising for future testing, but further attempts to tune layer types, quantity and parameters resulted in accuracy values still close to 0.54, which suggests that selection or tuning of models may not be the best approach to improve results.

The 14 models used in this study represent a very small sample of all possible model architectures using the chosen layer types. 

XXXXXXXXXXXX

\section{Future work}
The models required short training times and few examples to approach their limit.
This suggests that some patterns were very easy to find but they were insufficient to achieve a higher accuracy.
This raises the question whether there are harder patterns that could be found by a better model not yet tried or if this a more fundamental issue that would not be solved by a trial and error approach of tweaking of parameters and layer modifications.

Based on the work presented here, further studies focused on error analysis are in progress to address this question.